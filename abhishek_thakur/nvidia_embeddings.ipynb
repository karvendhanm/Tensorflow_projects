{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_model(data, catacols):\n",
    "    '''\n",
    "\n",
    "    :param data:\n",
    "    :param catacols:\n",
    "    :return:\n",
    "    '''\n",
    "    # initializing list of inputs\n",
    "    inputs = []\n",
    "\n",
    "    # initializing list of outputs\n",
    "    outputs = []\n",
    "\n",
    "    for col in catacols:\n",
    "        # find the number of unique values in the column\n",
    "        num_unique_vals = int(data[col].nunique())\n",
    "\n",
    "        # simple dimension of embedding calculator\n",
    "        # min. size is half of the number of unique values\n",
    "        # max. size is 50. max size depends on the number of unique\n",
    "        # categories too. Usually, 50 is quite sufficient but if you have\n",
    "        # millions of unique values, you might need a larger dimension.\n",
    "        embed_dim = int(min(np.ceil(num_unique_vals/2), 50))\n",
    "\n",
    "        # simple keras input layer with size 1\n",
    "        inp = layers.Input(shape=(1,))\n",
    "\n",
    "        # add embedding layer to raw input\n",
    "        # embedding size is always 1 more than unique values in input\n",
    "        out = layers.Embedding(\n",
    "            num_unique_vals + 1, embed_dim, name=col\n",
    "        )(inp)\n",
    "\n",
    "        # 1-d spatial dropout is the standard for embedding layers\n",
    "        #  you can use it in NLP tasks too.\n",
    "        out = layers.SpatialDropout1D(0.3)(out)\n",
    "\n",
    "        # reshape the input to the dimension of embedding\n",
    "        # this becomes our output layer for current feature\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "\n",
    "        # add input to input list\n",
    "        inputs.append(inp)\n",
    "\n",
    "        # add output to output list\n",
    "        outputs.append(out)\n",
    "\n",
    "    # concatenate all output layers\n",
    "    x = layers.Concatenate()(outputs)\n",
    "\n",
    "    # add a batchnorm layer.\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # a bunch of dense layers with dropout.\n",
    "    x = layers.Dense(300, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(300, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # using softmax and treating it as a two class problem\n",
    "    # you can also use sigmoid, then u need to use only one output class\n",
    "    y = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # create final model\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "\n",
    "    # compile the model\n",
    "    # we use adam and cross entropy\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run(_fold):\n",
    "    '''\n",
    "\n",
    "    :param _fold:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # load the cat_in_the_dat data\n",
    "    df = pd.read_csv('./data/cat_in_the_dat_train_folds.csv')\n",
    "\n",
    "    # list of all input features/predictors.\n",
    "    features = [col\n",
    "                for col in df.columns\n",
    "                if col not in ['id', 'target', 'kfold']\n",
    "                ]\n",
    "\n",
    "    # initializing label encoder\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "\n",
    "    # fill all the numm values with NONE\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna('NONE')\n",
    "        df.loc[:, col] = lbl.fit_transform(df[col])\n",
    "\n",
    "    # train and test data\n",
    "    df_train = df.loc[df['kfold'] != _fold, :]\n",
    "    df_valid = df.loc[df['kfold'] == _fold, :]\n",
    "\n",
    "    model = create_model(df, features)\n",
    "\n",
    "    # our features are list of lists\n",
    "    xtrain = [\n",
    "        df_train[features].values[:, k] for k in range(len(features))\n",
    "    ]\n",
    "    xvalid = [\n",
    "        df_valid[features].values[:, k] for k in range(len(features))\n",
    "    ]\n",
    "\n",
    "    #fetch target columns\n",
    "    ytrain = df_train.target.values\n",
    "    yvalid = df_valid.target.values\n",
    "\n",
    "    # convert target columns to categories\n",
    "    ytrain_cat = utils.to_categorical(ytrain)\n",
    "    yvalid_cat = utils.to_categorical(yvalid)\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(xtrain,\n",
    "              ytrain_cat,\n",
    "              validation_data=(xvalid, yvalid_cat),\n",
    "              verbose=1,\n",
    "              batch_size=1024,\n",
    "              epochs=3\n",
    "              )\n",
    "\n",
    "    # generate validation predictions\n",
    "    valid_preds = model.predict(xvalid)[:, 1]\n",
    "\n",
    "    # print roc auc score\n",
    "    auc = metrics.roc_auc_score(yvalid, valid_preds)\n",
    "\n",
    "    print(f'fold: {_fold}, auc: {auc}')\n",
    "\n",
    "    # clear session to free up GPU memory\n",
    "    K.clear_session()\n",
    "\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
